{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Ali Emre Öz </div>\n",
    "<div style=\"text-align: right\"> 213950785 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">EECS 461 - MACHINE LEARNING PROJECT FINAL REPORT</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Algorithms and Performance Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification algorithm I use: Naive Bayes Classifier, Random Forest Classifier, Logistic Regression and Ada Boost Classifier. \n",
    "\n",
    "For performance measurement, I use Hamming Lose which is the fraction of labels that are incorrectly predicted. According to scikit-learn documentation: \"In multiclass classification, the Hamming loss correspond to the Hamming distance between y_true and y_pred\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import AdaBoostClassifier as AdaBoost\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_csv(\"x_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"x_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_csv(\"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv(\"y_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_test[\"Unnamed: 0\"]\n",
    "del x_train[\"Unnamed: 0\"]\n",
    "del y_test[\"Unnamed: 0\"]\n",
    "del y_train[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.values\n",
    "x_test = x_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Naive Bayes Classifier\n",
    "\n",
    "\"Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Hamming Loss of NB is 0.792222484734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = OneVsRestClassifier(GaussianNB(), n_jobs=-1)\n",
    "nb.fit(x_train, y_train)\n",
    "nb_pred = nb_mdl.predict(x_test)\n",
    "hl = hamming_loss(y_test, nb_pred)\n",
    "print \"Naive Bayes\"\n",
    "print \"Hamming Loss of NB is \" + str(hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from hamming loss, nearly %80 of predictions are wrong. It is not a good result and we need to improve it with other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Hamming Loss of LR is 0.300182647572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_mdl = OneVsRestClassifier(LogisticRegression(class_weight='balanced', C=100, solver='sag', max_iter = 2500), n_jobs=-1)\n",
    "lr_mdl.fit(x_train, y_train)\n",
    "lr_pred = lr_mdl.predict(x_test)\n",
    "hl = hamming_loss(y_test, lr_pred)\n",
    "print \"Logistic Regression\"\n",
    "print \"Hamming Loss of LR is \" + str(hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from hamming loss, nearly %30 of predictions are wrong. It is better than naive-bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with PCA\n",
      "Hamming Loss of LR with PCA is 0.304294489677\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(x_train)\n",
    "var90pcs = len(pca.explained_variance_ratio_[np.cumsum(pca.explained_variance_ratio_)<.95])\n",
    "# reduce data using PCA\n",
    "X_train_reduced = pca.transform((x_train))\n",
    "X_test_reduced = pca.transform((x_test))\n",
    "lr_mdl_pca = OneVsRestClassifier(LogisticRegression(class_weight='balanced', C=100, solver='sag', max_iter = 5000), n_jobs=-1)\n",
    "lr_mdl_pca.fit(X_train_reduced[:,0:var90pcs], y_train)\n",
    "lr_pred_pca = lr_mdl_pca.predict(X_test_reduced[:,0:var90pcs])\n",
    "hl = hamming_loss(y_test, lr_pred_pca)\n",
    "\n",
    "print \"Logistic Regression with PCA\"\n",
    "print \"Hamming Loss of LR with PCA is \" + str(hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we apply PCA, the hamming loss was not change. However, the speed of our algorithm were improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Hamming Loss of RF is 0.128712016575\n"
     ]
    }
   ],
   "source": [
    "rfc_mdl = RFC(n_estimators=50, max_depth=25, class_weight ='balanced', n_jobs=-1).fit(x_train,y_train)\n",
    "rf_pred = rfc_mdl.predict(x_test)\n",
    "hl = hamming_loss(y_test, rf_pred)\n",
    "print \"Random Forest\"\n",
    "print \"Hamming Loss of RF is \" + str(hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF Classifier will get a good result when we compare it with previous algorithms. It predict %87 of the data true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Random Forest Classifier with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with PCA\n",
      "Hamming Loss of RF with PCA is 0.107235024716\n"
     ]
    }
   ],
   "source": [
    "rfc_mdl_pca = RFC(random_state = 0).fit(X_train_reduced[:,0:var90pcs],y_train)\n",
    "pred_pca = rfc_mdl_pca.predict(np.array(X_test_reduced[:,0:var90pcs]))\n",
    "hl = hamming_loss(y_test, pred_pca)\n",
    "print \"Random Forest with PCA\"\n",
    "print \"Hamming Loss of RF with PCA is \" + str(hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply PCA, we see that the hamming loss will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. ADABoost Classifier\n",
    "\n",
    "\"An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\" from scikit-learn documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Classifier for MultiClass Prediction with SAMME\n",
      "Hamming Loss of Adaboost Classifier for MultiClass Prediction with SAMME is 0.100465251527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ada_clf = OneVsRestClassifier(AdaBoostClassifier(DecisionTreeClassifier(max_depth=9), n_estimators=50, algorithm=\"SAMME\", learning_rate=0.5))\n",
    "ada_clf.fit(x_train, y_train)\n",
    "ada_clf_pred = ada_clf.predict(x_test)\n",
    "hl = hamming_loss(y_test, ada_clf_pred)\n",
    "print \"Adaboost Classifier for MultiClass Prediction with SAMME\"\n",
    "print \"Hamming Loss of Adaboost Classifier for MultiClass Prediction with SAMME is \" + str(hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the hamming distance, we can understand why the ensemble gave better result. It is the best result for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I have a larger set of data, or if I can use the director, actor, and screenwriter knowledge of the films, I can get better results.\n",
    "\n",
    "As a further enhancement, posters from films could also be used as a good feature.\n",
    "\n",
    "Besides, we can improve on the estimates by trying to predict more than one kind of films like multilabel classification, rather than predicting a single species."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
